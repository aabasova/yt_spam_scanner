{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import csv\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all CSV files in one file\n",
    "\n",
    "# Location of CSV files\n",
    "location = \"../data/csv/separate/\"\n",
    "\n",
    "# List all files in the location specified\n",
    "file_list = os.listdir(location)\n",
    "\n",
    "# Merge all CSV files\n",
    "df_concat = pd.concat([pd.read_csv(\"../data/csv/separate/\" + f).fillna(value = 0) for f in file_list ], ignore_index=True)\n",
    "\n",
    "# Save the combined files in on CSV file\n",
    "df_concat.to_csv(\"../data/csv/combined/combined.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the CVS file to an Elasticsearch json compatible format before uploading to Elasticsearch\n",
    "with open(\"../data/csv/combined/combined.csv\", \"r\", encoding='utf-8') as infile, open(\"../data/json/combined.json\", 'w+', encoding='utf-8') as outfile:\n",
    "\n",
    "    inreader = csv.reader(infile, delimiter=',', quotechar='\"')\n",
    "    \n",
    "    for id, line in enumerate(inreader):\n",
    "\n",
    "        # Ignore the columns names\n",
    "        if id == 0:\n",
    "            continue\n",
    "        index_line = {\"index\":{\"_id\":str(id)}}\n",
    "        document = OrderedDict()\n",
    "        document['comment_id'] = line[0]\n",
    "        document['author'] = line[1]\n",
    "        document['date'] = line[2]\n",
    "        document['content'] = line[3]\n",
    "\n",
    "        # Remove Byte Order Mark\n",
    "        if document['content'][-1] == '\\ufeff':\n",
    "            document['content'] = document['content'][:-2]\n",
    "        document['class'] = line[4]\n",
    "        json.dump(index_line, outfile, ensure_ascii=False)\n",
    "        outfile.write(\"\\n\")\n",
    "        json.dump(document, outfile, ensure_ascii=False)\n",
    "        outfile.write(\"\\n\")\n",
    "\n",
    "\n",
    "# Push the json file for Elasticsearch using the command below:\n",
    "# curl -u <login>:<password> -H \"Content-Type:application/x-ndjson\" -XPOST http://localhost:9200/comments/_bulk --data-binary \"@./data/json/combined.json\"\n",
    "# Also in Windows: \n",
    "# Invoke-RestMethod \"http://localhost:9200/comments/_bulk\" -Method Post -ContentType \"application/x-ndjson\" -InFile \"./data/json/combined.json\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a.almasri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\elasticsearch\\connection\\base.py:200: ElasticsearchWarning: [types removal] Specifying types in bulk requests is deprecated.\n",
      "  warnings.warn(message, category=ElasticsearchWarning)\n"
     ]
    },
    {
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
>>>>>>> 826fb2e4fa1d360097f136be1dbad5f1e25c5639
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Finished\n"
     ]
    }
   ],
   "source": [
    "# Direct upload from CSV to Elasticsearch\n",
    "\n",
    "\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch import Elasticsearch\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "# Set variables\n",
    "es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "index = 'comments'\n",
    "\n",
    "actions = []\n",
    "\n",
    "with open(\"./data/csv/combined/combined.csv\", \"r\", encoding='utf-8') as infile:\n",
    "  inreader = csv.reader(infile, delimiter=',', quotechar='\"')\n",
    "  for id, line in enumerate(inreader):\n",
    "        # Ignore the columns names\n",
    "        if id == 0:\n",
    "            continue \n",
    "\n",
    "        content = line[3]\n",
    "        # Remove Byte Order Mark\n",
    "        if content[-1] == '\\ufeff':\n",
    "          content = content[0:-2] \n",
    "\n",
    "        source = {'comment_id' : line[0],\n",
    "                  'author'     : line[1],\n",
    "                  'date'       : line[2],\n",
    "                  'content'    : line[3],\n",
    "                  'class'      : line[4]\n",
    "                }\n",
    "        action = {\n",
    "            \"_index\":   'comments',\n",
    "            '_op_type': 'index',\n",
    "            \"_type\":    '_doc',\n",
    "            \"_id\":       id,\n",
    "            \"_source\":   source\n",
    "                }\n",
    "        actions.append(action)    \n",
    "        \n",
    "  helpers.bulk(es_client, actions)\n",
    "  \n",
    "\n",
    "print('All Finished')        \n",
    " \n",
    "       \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19e2b0ba2fb85cf63582a4dff9b7969e3117b3bb4abcc9d2ff6cd82e4f2230b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
