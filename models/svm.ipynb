{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aa65c3c-59d3-4cda-a0fd-7d2fc421b1d2",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaa11e2",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9750c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt # graphs\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import joblib\n",
    "import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import sklearn\n",
    "#from sklearn.utils import resample # downsample dataset\n",
    "from sklearn.model_selection import train_test_split # split to training and testing datasets\n",
    "from sklearn.model_selection import GridSearchCV # cross validation\n",
    "#from sklearn.preprocessing import scale # scale and center data\n",
    "from sklearn.svm import SVC # support vector classifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db08491",
   "metadata": {},
   "source": [
    "### Import data\n",
    "\n",
    "We work with 1956 comments from 5 different YouTube videos. The [YouTube Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/YouTube+Spam+Collection#) is freely available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccee94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"data/YouTube-Spam-Collection/\"\n",
    "files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "corpus = pd.concat((pd.read_csv(file) for file in files), ignore_index=True)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ee6a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(corpus) == 1956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc218e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download(\"wordnet\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776d133",
   "metadata": {},
   "source": [
    "### Statistics about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db72209",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data types:\\n{corpus.dtypes}\\n\")\n",
    "print(f\"There are {len(corpus['CLASS'].unique())} comment types: {corpus['CLASS'].unique()}\")\n",
    "print(f\"The dataset contains of {len(corpus)} examples: {len(corpus.loc[corpus['CLASS'] == 1])} spam and {len(corpus.loc[corpus['CLASS'] == 0])} legitimate comments\")\n",
    "\n",
    "corpus.groupby(\"CLASS\").CONTENT.count().plot.bar(ylim=0)\n",
    "plt.xticks([0,1],['legitimate', 'spam'], rotation=0)\n",
    "plt.xlabel(\"comment type\")\n",
    "plt.ylabel(\"numberof comments\")\n",
    "plt.title(\"Dataset distribution\", pad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e249a60d",
   "metadata": {},
   "source": [
    "### Peprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('omw-1.4')\n",
    "    \n",
    "def preprocess_data(corpus,\n",
    "                    irrelevant_features=[\"COMMENT_ID\", \"AUTHOR\", \"DATE\"],\n",
    "                    #rename_columns={\"CONTENT\":\"COMMENT\"}\n",
    "                   ):\n",
    "    \n",
    "    # drop irrelevant features\n",
    "    corpus.drop(irrelevant_features, inplace=True, axis=1)\n",
    "\n",
    "    # remove blank rows if any\n",
    "    corpus.dropna()\n",
    "    \n",
    "    # add column for representation\n",
    "    corpus['REPR'] = corpus.loc[:, 'CONTENT']\n",
    "        \n",
    "    # lower case\n",
    "    corpus['REPR'] = corpus['REPR'].str.lower()\n",
    "\n",
    "    # change column name\n",
    "    #for old, new in rename_columns:\n",
    "        #corpus.rename({old : new}, axis=1, inplace=True)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    \n",
    "    for comment in corpus[\"REPR\"]:    \n",
    "        comment = nltk.word_tokenize(comment) # tokenizing nltk.WordPunctTokenizer().tokenize(comment.lower())?\n",
    "        comment = [lemmatizer.lemmatize(word) for word in comment] # lemmatizing\n",
    "        comment = [word for word in comment if word not in stop_words] # removing stopwords\n",
    "        comment = \" \".join(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data(corpus)\n",
    "\n",
    "# binary feature representation\n",
    "vectorizer = CountVectorizer(binary=True, max_df=0.95) #max_features=10000, tokenizer=lambda doc: doc)\n",
    "BOW = vectorizer.fit_transform(corpus[\"REPR\"])\n",
    "\n",
    "# count based feature representation\n",
    "vectorizer_2 = CountVectorizer(binary=False, max_df=0.95) #max_features=10000)\n",
    "BOW_2 = vectorizer_2.fit_transform(corpus[\"REPR\"])\n",
    "\n",
    "# bag of 2-Grams\n",
    "bigram_vectorizer = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[2,2])\n",
    "BOW_3 = bigram_vectorizer.fit_transform(corpus[\"REPR\"])\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, use_idf=True, stop_words='english') \n",
    "#use_idf=True, min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
    "tfidf_vectorizer.fit(corpus[\"REPR\"])\n",
    "corpus_tfidf = tfidf_vectorizer.transform(corpus[\"REPR\"])\n",
    "\n",
    "print(\"Formula String: \",corpus[\"REPR\"][0])\n",
    "embedding = corpus_tfidf[0].toarray()\n",
    "print(len(embedding[0]))\n",
    "print(\"Vector Representation: \",embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39aed4d5",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd0a59",
   "metadata": {},
   "source": [
    "### Train a SVM, save the model and report the classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ec61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, labels):\n",
    "    return train_test_split(features,labels, test_size=0.3,random_state=42,shuffle=True)\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "    now = datetime.datetime.now()\n",
    "    model_output_path = \"saved_models/\"+model.__class__.__name__.lower()+\"_\"+str(now.minute)+\"-\"+str(now.second)+\".joblib\"\n",
    "    joblib.dump(model, open(model_output_path, 'wb+'))\n",
    "\n",
    "\n",
    "def report(model, best_model, X_test, y_test, labels):\n",
    "    print(f\"Best parameters set {model.best_params_} with accuracy {model.best_score_}\")\n",
    "    y_predict = best_model.predict(X_test)\n",
    "    \n",
    "    labels = sorted(list(set(labels)))\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_predict, labels=labels) # lables=clf.classes_\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_predict))\n",
    "    \n",
    "    print(\"\\nAccuracy: {}%\".format(accuracy_score(y_test, y_predict)*100))\n",
    "    print(\"Precision: {}%\".format(sklearn.metrics.precision_score(y_test, y_predict)*100))\n",
    "    print(\"Recall: {}%\".format(sklearn.metrics.recall_score(y_test, y_predict)*100))\n",
    "\n",
    "    \n",
    "def train_svm_classifier(features, labels):\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(features, labels)\n",
    "\n",
    "    param = {'C': [0.1, 1, 10, 100, 1000],\n",
    "             'gamma': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "             'kernel': ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "            }\n",
    "\n",
    "    clf = GridSearchCV(SVC(), param, cv=5, n_jobs=2, verbose=0)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    svm_clf = clf.best_estimator_\n",
    "    \n",
    "    # save model to disk\n",
    "    save_model(svm_clf)\n",
    "    \n",
    "    # show test report\n",
    "    #report(clf, svm_clf, X_test, y_test, labels)\n",
    "    \n",
    "    return svm_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd26b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = train_svm_classifier(BOW, np.asarray(corpus[\"CLASS\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78519350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = train_svm_classifier(BOW_2, np.asarray(corpus[\"CLASS\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49643b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = train_svm_classifier(BOW_3, np.asarray(corpus[\"CLASS\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2c148",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fe675",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_svm_classifier(corpus_tfidf, np.asarray(corpus[\"CLASS\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f46de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_single_comment(comment):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "\n",
    "    comment = comment.lower()\n",
    "    comment = nltk.word_tokenize(comment) # tokenizing\n",
    "    comment = [lemmatizer.lemmatize(word) for word in comment] # lemmatizing\n",
    "    comment = [word for word in comment if word not in stop_words] # removing stopwords\n",
    "    print(comment)\n",
    "    print(len(comment))\n",
    "    print(\" \".join(comment))\n",
    "    return \" \".join(comment)\n",
    "\n",
    "def get_comment_embedding(comment):\n",
    "    to_be_return = tfidf_vectorizer.transform([preprocess_single_comment(comment)])\n",
    "    print(\"to be returned \", type(to_be_return))\n",
    "    return to_be_return\n",
    "\n",
    "voc_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "print(voc_tfidf)\n",
    "print(len(voc_tfidf))\n",
    "print(model.get_params())\n",
    "\n",
    "comment_str = \"4:20 one of a very diverse set of clips, but don't know what it is. Is it a walking plant?  Thanks Stern. And another one at 20:06!\"\n",
    "comment_str2 = \"Go to my page\"\n",
    "print(type(model))\n",
    "prediction_label = model.predict(get_comment_embedding(comment_str))\n",
    "prediction_label2 = model.predict(get_comment_embedding(comment_str2))\n",
    "print(prediction_label, len(prediction_label))\n",
    "print(prediction_label2, len(prediction_label2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6e25f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75c7045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6eb847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec  7 2022, 01:12:00) [GCC 9.4.0]"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "vscode": {
   "interpreter": {
    "hash": "5907587528920898ec295df888b65059550f6aac43dddaf75e34466408980fac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
